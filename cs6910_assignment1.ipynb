{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqQMORViBnNvuIav2wkEz9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sid-betalol/CS6910-FODL-Assignment1/blob/main/cs6910_assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To add\n",
        "1. More activation functions: LeakyRelu\n",
        "2. Given optimizers and more(eve)\n",
        "3. class/function to make a network\n",
        "4. adding wandb logger\n",
        "5. breaking the notebook down to scripts based on code implemengtation instructions\n",
        "6. dropuout and early stopping if possible"
      ],
      "metadata": {
        "id": "rsp6oW-6BWSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**wandb setup**"
      ],
      "metadata": {
        "id": "UXBbDC-D1uSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "d2bVSz0P1tq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "5Zj3_pOH2rHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Importing the required libraries and dataset**\n"
      ],
      "metadata": {
        "id": "x6kDAiJ-rLaO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubdkSrjvntI_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Getting and exploring the data**"
      ],
      "metadata": {
        "id": "5A1m7ilV-zuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "np.random.seed(42)\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "sgnvTZiMp0ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train Data:')\n",
        "print('X:', x_train.shape)\n",
        "print('Y:', y_train.shape)\n",
        "print()\n",
        "print('Test Data:')\n",
        "print('X:', x_test.shape)\n",
        "print('Y:', y_test.shape)"
      ],
      "metadata": {
        "id": "nMF5GY30_AlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Classwise Sample from data**"
      ],
      "metadata": {
        "id": "k0vl8lRbkOVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train is used instead of y_test, as training data is expected\n",
        "# to have samples from every class unlike the test data\n",
        "num_labels = np.unique(y_train).shape[0]\n",
        "# print(num_labels) ## prints 10 in accordance with the keras dataset\n",
        "labels = [\n",
        "    'T-shirt/top', \n",
        "    'Trouser', \n",
        "    'Pullover', \n",
        "    'Dress', \n",
        "    'Coat', \n",
        "    'Sandal', \n",
        "    'Shirt', \n",
        "    'Sneaker', \n",
        "    'Bag', \n",
        "    'Ankle boot',\n",
        "    ]\n",
        "def show_samples(n, X = x_train, Y = y_train, n_classes = num_labels, classes = labels):\n",
        "    # n : number of samples to be shown from each class\n",
        "    \n",
        "    # samples divided by class\n",
        "    labelled_data = {i: x_train[y_train==i] for i in range(n_classes)}\n",
        "    \n",
        "    # maximum number of available samples in each class  \n",
        "    max_samples = {i:len(labelled_data[i]) for i in range(n_classes)}\n",
        "\n",
        "    # samples from class to be shown in random order\n",
        "    shuffled_indices = {i: np.random.permutation(max_samples[i]) for i in range(n_classes)}\n",
        "    \n",
        "    # maximum number of available samples will be shown if the user \n",
        "    # asks for more samples than available to be shown\n",
        "    num_samples = {i: min(max_samples[i], n) for i in range(n_classes)}\n",
        "\n",
        "    fig, axs = plt.subplots(nrows = 10, ncols = max(num_samples.values()), figsize=(1.2*max(num_samples.values()), 15))\n",
        "    for i in range(10):\n",
        "        for j in range(num_samples[i]):\n",
        "            img = labelled_data[i][shuffled_indices[i][j]].astype(np.uint8).reshape(28, 28)\n",
        "            axs[i, j].imshow(img, cmap=\"gray\")\n",
        "            axs[i, j].axis(\"off\")\n",
        "            axs[i, j].set_title(classes[i])\n",
        "    plt.show()\n",
        "\n",
        "show_samples(15)"
      ],
      "metadata": {
        "id": "rgkZps0_jZMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Template Class for Activation Functions**"
      ],
      "metadata": {
        "id": "laYjy3tu38b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class daddyActivation():\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \"\"\"\n",
        "        self.grads = {}\n",
        "        self.backprop_cache = {}\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Calling the class as a function instance\n",
        "        does a forward and a backward pass\n",
        "        \"\"\"\n",
        "        op = self.forward(*args, **kwargs)\n",
        "        self.grads = self.calc_grads(*args, **kwargs)\n",
        "        return op\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Defining the forward pass of the activation function\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def calc_grads(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Calculates the gradient of the activation function\n",
        "        with respect to its input\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def backward(self, *args, **kwargs):\n",
        "        \"\"\" \n",
        "        Calculates the gradients of the loss with respect \n",
        "        to the input of the activation function, using the gradients \n",
        "        computed in the calc_grads method\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "CU2DS6L1m9zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Activation Functions**"
      ],
      "metadata": {
        "id": "gxsAOGqd_grh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid(daddyActivation):\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        self.backprop_cache = 1/(1+np.exp(-x))\n",
        "        return self.backprop_cache\n",
        "\n",
        "    def calc_grads(self, x):\n",
        "        \n",
        "        id = \"x\"\n",
        "        y = self.backprop_cache\n",
        "        diff = y*(1-y)\n",
        "        return {id:diff}\n",
        "\n",
        "    def backward(self, y_hat):\n",
        "        \n",
        "        return self.grads['x']*y_hat\n",
        "\n",
        "\n",
        "class Tanh(daddyActivation):\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        self.backprop_cache = (np.exp(x) - np.exp(-x))/(np.exp(-x)+np.exp(x))\n",
        "        return self.backprop_cache\n",
        "\n",
        "    def calc_grads(self, x):\n",
        "        \n",
        "        id = \"x\"\n",
        "        y = self.backprop_cache\n",
        "        diff = 1- y**2\n",
        "        return {id:diff}\n",
        "\n",
        "    def backward(self, y_hat):\n",
        "        \n",
        "        return self.grads['x']*y_hat\n",
        "\n",
        "\n",
        "class ReLU(daddyActivation):\n",
        "\n",
        "    def __init__(self):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        self.backprop_cache = np.maximum(x, 0.0)\n",
        "        return self.backprop_cache\n",
        "\n",
        "    def calc_grads(self,x):\n",
        "        \n",
        "        id = \"x\"\n",
        "        y = self.backprop_cache\n",
        "        diff = (y > 0).astype(\"float\")\n",
        "        return {id:diff}\n",
        "\n",
        "    def backward(self, y_hat):\n",
        "        \n",
        "        return self.grads['x']*y_hat\n",
        "\n",
        "\n",
        "class LeakyReLU(daddyActivation):\n",
        "    \n",
        "    def __init__(self, alpha=0.1):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.backprop_cache = np.maximum(x, self.alpha*x)\n",
        "        return self.backprop_cache\n",
        "    \n",
        "    def calc_grads(self, x):\n",
        "        id = \"x\"\n",
        "        y = self.backprop_cache\n",
        "        diff = np.where(y > 0, 1, self.alpha)\n",
        "        return {id: diff}\n",
        "    \n",
        "    def backward(self, y_hat):\n",
        "        return self.grads['x']*y_hat\n",
        "\n",
        "class ParamReLU(daddyActivation):\n",
        "    \n",
        "    def __init__(self, alpha=0.1):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.backprop_cache = np.maximum(x, self.alpha*x)\n",
        "        return self.backprop_cache\n",
        "    \n",
        "    def calc_grads(self, x):\n",
        "        id = \"x\"\n",
        "        y = self.backprop_cache\n",
        "        diff = np.where(y > 0, 1, self.alpha)\n",
        "        self.grads[\"alpha\"] = np.where(y > 0, 0, x*self.alpha)\n",
        "        return {id: diff}\n",
        "    \n",
        "    def backward(self, y_hat):\n",
        "        self.grads[\"x\"] = self.grads[\"x\"] + self.grads[\"alpha\"]*self.alpha\n",
        "        return self.grads[\"x\"]*y_hat"
      ],
      "metadata": {
        "id": "CvyVwHGJ_khG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Template Class for Loss Functions**"
      ],
      "metadata": {
        "id": "rUlqWLbBkXcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class daddyLoss():\n",
        "    \n",
        "    def __init__(self, *args, **kwargs):\n",
        "        \n",
        "        self.grads = {}\n",
        "        self.backprop_cache = {}\n",
        "\n",
        "    def __call__(self, y_pred, y_true, *args, **kwargs):\n",
        "        \n",
        "        op = self.forward(y_pred, y_true, *args, **kwargs)\n",
        "        self.grads = self.calc_grads(y_pred, y_true, *args, **kwargs)\n",
        "        return op\n",
        "\n",
        "    def forward(self, y_pred, y_true, *args, **kwargs):\n",
        "        \n",
        "        pass\n",
        "\n",
        "    def calc_grads(self, y_pred, y_true, *args, **kwargs):\n",
        "        \n",
        "        pass\n",
        "\n",
        "    def backward(self, *args, **kwargs):\n",
        "        \n",
        "        return self.grads['x']\n"
      ],
      "metadata": {
        "id": "0N-qVwkxkWc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Helper Functions for Loss**"
      ],
      "metadata": {
        "id": "Jpn3O1KiK0Wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# classification problem, so outputs can be represented as\n",
        "# one-hot encoded vectors\n",
        "\n",
        "def one_hot_encode(y, num_classes):\n",
        "    \n",
        "    encoding = np.zeros((len(y), num_classes), dtype = int)\n",
        "    encoding[np.arange(len(y)), y] = 1\n",
        "    return encoding\n",
        "\n",
        "# softmax will be used for the output layer\n",
        "\n",
        "def softmax(x):\n",
        "    \n",
        "    exp_x = np.exp(x - np.max(x, axis= -1, keepdims=True))\n",
        "    return exp_x/ np.sum(exp_x, axis = -1, keepdims=True)"
      ],
      "metadata": {
        "id": "-ko_4bhQLBOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Loss Functions**"
      ],
      "metadata": {
        "id": "XadNpcKoJhA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MSE(daddyLoss):\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \n",
        "        num_classes = y_pred.shape[-1]\n",
        "        probabs = softmax(y_pred)\n",
        "        y_true_encoding = np.eye(num_classes, dtype = int)[np.array(y_true).astype(int)]\n",
        "        self.backprop_cache['y_true'] = y_true_encoding\n",
        "        loss = np.mean(np.sum((probabs - y_true_encoding)**2, axis=1))\n",
        "        self.backprop_cache['probabs'] = probabs\n",
        "        return loss\n",
        "\n",
        "    def calc_grads(self, y_pred, y_true):\n",
        "        \n",
        "        batch_size = y_pred.shape[0]\n",
        "        grad = 2*(self.backprop_cache[\"probabs\"] - self.backprop_cache[\"y_true\"])\n",
        "        grad = grad/batch_size\n",
        "        return {'x': grad}\n",
        "\n",
        "class LogLoss(daddyLoss):\n",
        "\n",
        "    def __init__(self):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \n",
        "        num_classes = y_pred.shape[-1]\n",
        "        probabs = softmax(y_pred)\n",
        "        y_true_encoding = np.eye(num_classes, dtype = int)[np.array(y_true).astype(int)]\n",
        "        self.backprop_cache['y_true'] = y_true_encoding\n",
        "        loss = np.mean(np.sum(- y_true_encoding * np.log(probabs), axis=1))\n",
        "        self.backprop_cache['probabs'] = probabs\n",
        "        return loss\n",
        "\n",
        "    def calc_grads(self, y_pred, y_true):\n",
        "        \n",
        "        batch_size = y_pred.shape[0]\n",
        "        grad = self.backprop_cache[\"probabs\"] - self.backprop_cache[\"y_true\"]\n",
        "        grad = grad/batch_size\n",
        "        return {'x':grad}"
      ],
      "metadata": {
        "id": "osa8tbHYItyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Template Class for Optimizers**"
      ],
      "metadata": {
        "id": "CkeWScoNsLQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class daddyOptimizer():\n",
        "    \n",
        "    def __init__(self, *args, **kwargs):\n",
        "        \n",
        "        self.history = {}\n",
        "        pass\n",
        "\n",
        "    def update_weights(self, layer, *args, **kwargs):\n",
        "        \n",
        "        update = self.calc_update(layer)\n",
        "        for k, v in layer.weights.items():\n",
        "            layer.weights[k] = layer.weights[k] + update[k]\n",
        "\n",
        "    def calc_update(self, layer, *args, **kwargs):\n",
        "        #needs to be overloaded based on the optimizer\n",
        "        pass"
      ],
      "metadata": {
        "id": "T9CIiomunxay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Helper Functions"
      ],
      "metadata": {
        "id": "gxu22n_1-C7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_array(x):\n",
        "    return np.zeros(x.shape, dtype=x.dtype)"
      ],
      "metadata": {
        "id": "OkVSP3I7-Cam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Optimizers**\n",
        "SGD update rule: https://iitm-pod.slides.com/arunprakash_ai/cs6910-lecture-5#/0/56/7\n",
        "\n",
        "Momentum Update Rule: https://iitm-pod.slides.com/arunprakash_ai/cs6910-lecture-5#/0/40/7\n",
        "\n",
        "NAG Update Rule: https://iitm-pod.slides.com/arunprakash_ai/cs6910-lecture-5#/0/50/7\n",
        "\n",
        "Implementation issue with NAG Update rule, needs to be fixed\n",
        "\n",
        "RMSProp Update Rule: https://iitm-pod.slides.com/arunprakash_ai/cs6910-lecture-52#/0/18/5\n",
        "\n",
        "Adam Update Rule: https://iitm-pod.slides.com/arunprakash_ai/cs6910-lecture-52#/0/40/10\n",
        "\n",
        "NAdam Update Rule: https://iitm-pod.slides.com/arunprakash_ai/cs6910-lecture-52#/0/68\n",
        "\n",
        "Add a comment to remark about bias correction in Adam/NAdam\n",
        "\n",
        "Summary of optimizers: https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/"
      ],
      "metadata": {
        "id": "cWNHruzT_2vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD(daddyOptimizer):\n",
        "\n",
        "    def __init__(self, learning_rate = 0.01):\n",
        "\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def calc_update(self, layer):\n",
        "\n",
        "        update = {}\n",
        "        for k,v in layer.weights.items():\n",
        "            update[k] = -self.learning_rate*layer.del_theta[k]\n",
        "        return update\n",
        "\n",
        "class Momentum(daddyOptimizer):\n",
        "\n",
        "    # update rule:\n",
        "    # u_t = beta*u_[t-1] + eta*del(w_t)\n",
        "    # w_[t+1] = w_t - u_t\n",
        "\n",
        "    def __init__(self, learning_rate = 0.001, beta = 0.9):\n",
        "\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta = beta\n",
        "\n",
        "    def calc_update(self, layer):\n",
        "\n",
        "        update = {}\n",
        "\n",
        "        if self.history == {}:\n",
        "            for k, v in layer.weights.items():\n",
        "                self.history[k] = {}\n",
        "                self.history[k]['u'] = zero_array(v)\n",
        "        \n",
        "        for k, v in layer.weights.items():\n",
        "            self.history[k]['u'] = self.beta*self.history[k]['u'] + self.learning_rate*layer.del_theta[k]\n",
        "            update[k] = -self.history[k]['u']\n",
        "\n",
        "        return update\n",
        "\n",
        "class NAG(daddyOptimizer):\n",
        "\n",
        "    # update rule:\n",
        "    # u_t = beta*u_[t-1] + eta*del(w_t - beta*u_[t-1])\n",
        "\n",
        "    def __init__(self, learning_rate = 0.001, beta = 0.9):\n",
        "\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta = beta\n",
        "\n",
        "    def calc_update(self, layer):\n",
        "\n",
        "        update = {}\n",
        "\n",
        "        if self.history == {}:\n",
        "            for k, v in layer.weights.items():\n",
        "                self.history[k] = {}\n",
        "                self.history[k]['u'] = zero_array(v)\n",
        "                self.history[k]['prev_w'] = v.copy()\n",
        "\n",
        "        for k,v in layer.weights.items():\n",
        "            prev_w = self.history[k]['prev_w']\n",
        "            w = v - self.beta*self.history[k]['u']\n",
        "            layer.weights[k] = w\n",
        "\n",
        "            layer(*layer.backprop_cache['x'])\n",
        "            \n",
        "            self.history[k]['u'] = self.beta*self.history[k]['u'] + self.learning_rate*layer.del_theta[k]\n",
        "\n",
        "            update[k] = -self.history[k]['u']\n",
        "            layer.weights[k] = prev_w\n",
        "\n",
        "        return update\n",
        "\n",
        "class RMSProp(daddyOptimizer):\n",
        "\n",
        "    def __init__(self, learning_rate = 0.001, beta = 0.9, epsilon = 1e-7):\n",
        "\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def calc_update(self, layer):\n",
        "\n",
        "        update = {}\n",
        "        if self.history == {}:\n",
        "            for k, v in layer.weights.items():\n",
        "                self.history[k] = {}\n",
        "                self.history[k]['u'] = zero_array(v)\n",
        "\n",
        "        for k,v in layer.weights.items():\n",
        "            self.history[k]['u'] = self.beta*self.history[k]['u'] + (1 - self.beta)*(layer.del_theta[k]**2)\n",
        "            sqrt_term = np.sqrt(self.history[k]['u'] + self.epsilon)\n",
        "            update[k] = -(self.learning_rate*layer.weights[k]/sqrt_term)\n",
        "\n",
        "        return update\n",
        "\n",
        "class Adam(daddyOptimizer):\n",
        "\n",
        "    def __init__(self, learning_rate = 0.001, epsilon = 1e-7, beta1 = 0.9, beta2 = 0.999):\n",
        "\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.steps = 1\n",
        "\n",
        "    def calc_update(self, layer):\n",
        "\n",
        "        update = {}\n",
        "\n",
        "        if self.history == {}:\n",
        "            for k, v in layer.weights.items():\n",
        "                self.history[k] = {}\n",
        "                self.history[k]['u'] = zero_array(v)\n",
        "                self.history[k]['m'] = zero_array(v)\n",
        "\n",
        "        for k,v in layer.weights.items():\n",
        "            self.history[k]['m'] = self.beta1*self.history[k]['m'] + (1 - self.beta1)*layer.del_theta[k]\n",
        "            self.history[k]['u'] = self.beta2*self.history[k]['u'] + (1 - self.beta2)*(layer.del_theta[k]**2)\n",
        "\n",
        "            corrected_avg = self.history[k]['m']/(1-(self.beta1)**self.steps)\n",
        "            corrected_squared_avg = self.history[k]['m']/(1-(self.beta2)**self.steps)\n",
        "\n",
        "            sqrt_term = np.sqrt(corrected_squared_avg) + self.epsilon\n",
        "            update[k] = -(self.learning_rate*corrected_avg/sqrt_term)\n",
        "\n",
        "        self.steps+=1\n",
        "        return update\n",
        "\n",
        "class NAdam(daddyOptimizer):\n",
        "\n",
        "    def __init__(self, learning_rate = 0.001, epsilon = 1e-7, beta1 = 0.9, beta2 = 0.999):\n",
        "\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.steps = 1\n",
        "\n",
        "    def calc_update(self, layer):\n",
        "\n",
        "        update = {}\n",
        "\n",
        "        if self.history == {}:\n",
        "            for k, v in layer.weights.items():\n",
        "                self.history[k] = {}\n",
        "                self.history[k]['u'] = zero_array(v)\n",
        "                self.history[k]['m'] = zero_array(v)\n",
        "\n",
        "        for k,v in layer.weights.items():\n",
        "            self.history[k]['m'] = self.beta1*self.history[k]['m'] + (1 - self.beta1)*layer.del_theta[k]\n",
        "            self.history[k]['u'] = self.beta2*self.history[k]['u'] + (1 - self.beta2)*(layer.del_theta[k]**2)\n",
        "\n",
        "            corrected_avg = self.history[k]['m']/(1-(self.beta1)**self.steps)\n",
        "            corrected_squared_avg = self.history[k]['m']/(1-(self.beta2)**self.steps)\n",
        "\n",
        "            sqrt_term = np.sqrt(corrected_squared_avg) + self.epsilon\n",
        "            delta_coeff = (1-self.beta1)/(1-self.beta1**self.steps)\n",
        "\n",
        "            update[k] = -((self.learning_rate/sqrt_term)*(self.beta1*corrected_avg + delta_coeff*layer.del_theta[k]))\n",
        "\n",
        "        self.steps+=1\n",
        "        return update"
      ],
      "metadata": {
        "id": "2qizv9Jo_2R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Template Class for Layers**"
      ],
      "metadata": {
        "id": "AwGxKgDPVwg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class daddyLayer():\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        \n",
        "        self.grads = {}\n",
        "        self.weights = {}\n",
        "        self.backprop_cache = {}\n",
        "        self.optimizer = None\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "\n",
        "        op = self.forward(*args, **kwargs)\n",
        "        self.grads = self.calc_grads(*args, **kwargs)\n",
        "        return op\n",
        "\n",
        "    def init_weights(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def calc_grads(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def backward(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def update_weights(self, *args, **kwargs):\n",
        "        \n",
        "        self.optimizer.update_weights(self)"
      ],
      "metadata": {
        "id": "pBD1F-ogVwGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Layers of a Feedforward neural network**\n",
        "reference used for xavier and kaiming initialization: https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79"
      ],
      "metadata": {
        "id": "gXLzv-8d0mFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FNNLayer(daddyLayer):\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim, weight_decay = None, init_method = 'random'):\n",
        "\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.weight_decay = weight_decay\n",
        "        self.init_method = init_method\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \n",
        "        if self.init_method == 'random':\n",
        "            self.weights['w'] = np.random.randn(self.in_dim, self.out_dim)*np.sqrt(1/self.in_dim)\n",
        "            self.weights['b'] = np.random.randn(1, self.out_dim)*np.sqrt(1/self.in_dim)\n",
        "\n",
        "        elif self.init_method == 'xavier':\n",
        "            max = np.sqrt(6 / (self.in_dim + self.out_dim))\n",
        "            min = -max\n",
        "            self.weights['w'] = np.random.uniform(low = min, high = max, size = (self.in_dim, self.out_dim))\n",
        "            self.weights['b'] = np.random.uniform(low = min, high = max, size = (1, self.out_dim))\n",
        "\n",
        "        elif self.init_method == \"kaiming\":\n",
        "            self.weights[\"w\"] = np.random.randn(self.in_dim, self.out_dim) * np.sqrt(2 / self.in_dim)\n",
        "            self.weights[\"b\"] = np.random.randn(1, self.out_dim) * np.sqrt(2 / self.in_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        self.backprop_cache['x'] = x\n",
        "        op = np.einsum('ij,jk->ik', x, self.weights[\"w\"]) + self.weights[\"b\"]\n",
        "        return op\n",
        "\n",
        "    def calc_grads(self, x):\n",
        "\n",
        "        dels = {}\n",
        "        dels['w'] = np.einsum('ij -> ji', self.backprop_cache['x'])\n",
        "        dels['x'] = np.einsum('ij -> ji', self.weights['w'])\n",
        "        return dels\n",
        "\n",
        "    def backward(self, y_hat):\n",
        "\n",
        "        x_hat = np.einsum('ij,kj->ki', y_hat, self.grads[\"x\"])\n",
        "        w_hat = np.einsum('ij,ik->kj', self.grads[\"w\"], y_hat)\n",
        "        b_hat = np.sum(y_hat, axis=0, keepdims=True)\n",
        "        if self.weight_decay:\n",
        "            w_hat = w_hat + 2 * self.weight_decay * self.weights[\"w\"]\n",
        "            b_hat = b_hat + 2 * self.weight_decay * self.weights[\"b\"]\n",
        "        self.del_theta = {'w': w_hat, 'b': b_hat}\n",
        "        return x_hat\n",
        "\n",
        "    def update_weights(self):\n",
        "\n",
        "        self.optimizer.update_weights(self)"
      ],
      "metadata": {
        "id": "wvLupGE60xNy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}