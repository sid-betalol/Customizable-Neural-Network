# Custom Classes to create a Neural Network using NumPy
I have created classes which can be used to create a neural network architecture of your choice. You can use these classes to define your own layers and activation functions and use loss functions and optimizers of your own choice for different tasks.

Implementations:
1. Activation Functions: Sigmoid, TanH, ReLU, LeakyReLU
2. Loss Functions: Mean-squared error, LogLoss
3. Optimizers: Stochastic Gradient Descent, Monentum, Bengio Nesterov Momentum/NAG, RMSProp, Adam, NAdam
4. Feedforward Layers
5. Weight Initialization: Random, Xavier (functional implementaion)
6. Tested performance on the Fashion-MNIST Dataset with hyperparameter tuning using Wandb



This can be used as a reference of the tasks performed: https://wandb.ai/cs6910_2023/A1/reports/CS6910-Assignment-1--VmlldzozNTI2MDc5

I'll be updating this Wandb report as I carry out experiments: https://wandb.ai/sidbetala/cs6910-assignment1/reports/CS6910-Assignment-1--VmlldzozODMzNDQ2
